{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 第1章 深度学习介绍"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### 1.1 人工智能\n",
    "\n",
    "人工智能==机器智能"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### 1.2 数据挖掘、机器学习和深度学习\n",
    "\n",
    "- 数据挖掘：在大型的数据库中发现有用的信息，并加以分析的过程，也就是人们说的KDD。\n",
    "\n",
    "- 机器学习：实现人工智能的一种途径，对比于数据挖掘从大数据之间找到相互特性而言，机器学习更注重算法的设计，让计算机能够自动地从数据中“学习”规律，并利用规律对未知数据进行预测。\n",
    " - 监督学习\n",
    " - 无监督学习\n",
    " - 半监督学习\n",
    " - 迁移学习\n",
    " - 增强学习\n",
    "\n",
    "- 深度学习：最初版本是人工神经网络，是机器学习的一个分支。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### 1.3 学习资源与建议\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 第2章 深度学习框架"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### 2.1 深度学习框架介绍\n",
    "\n",
    "- Tensorflow：Google开源，使用C++语言开发的开源数学计算软件，使用数据流图(Data Flow Graph)的形式进行计算。图中的节点代表数学运算，而图中的线条表示多维数据数组(tensor)之间的交互。由于其语言太多余底层，目前有很多基于Tensorflow的第三方抽象库将Tensorflow的函数进行封装，使其变得简洁，目前比较有名的几个是Keras、Tflean、tfslim，以及TensorLayer。\n",
    "\n",
    "- Caffe：伯克利的贾扬清，用C++写的，没有提供Python接口，只提供了C++的接口。缺点是不够灵活，同时内存占用高。\n",
    "\n",
    "- Theano：蒙特利尔理工学院，核心是一个数学表达式的编译器。\n",
    "\n",
    "- Torch：有大量机器学习算法支持的科学计算框架，特点在于特别灵活，采用编程语言Lua。\n",
    "\n",
    "- MXNet：李沐，亚马逊的官方框架，有着非常好的分布式支持，而且性能特别好，占用显存低，同时期开放的语言接口不仅仅有Python和C++，还有R、Matlab、Scala、JavaScript，等等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### 2.2 PyTorch介绍\n",
    "\n",
    "PyTorch使用了Python作为开发语言，不仅能够实现强大的GPU加速，同时还支持动态神经网络，这事现在很多主流框架比如Tensorflow等都不支持的。  \n",
    "\n",
    "Tensorflow与Caffe都是命令式的编程语言，而且是静态的，首先必须构建一个神经网络，然后一次又一次使用同样的结构，如果想要改变网络的结构，就必须从头开始。但是对于PyTorch，通过一种反向自动求导的技术，可以让你零延迟地任意改变神经网络的行为。  \n",
    "\n",
    "PyTorch的代码相对于Tensorflow而言，更加简洁直观，源代码更友好，更容易看懂。  \n",
    "\n",
    "PyTorch的特点：  \n",
    "- 支持GPU\n",
    "- 动态神经网络\n",
    "- Python优先\n",
    "- 命令式体验\n",
    "- 轻松扩展"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### 2.3 配置PyTorch深度学习环境\n",
    "\n",
    "PyTorch 的安装非常方便，可以使用 Anaconda 进行安装，也可以使用 pip 进行安装，比如\n",
    "\n",
    "使用 conda 进行安装   \n",
    "`conda install pytorch torchvision -c pytorch`\n",
    "\n",
    "或者使用 pip   \n",
    "`pip install http://download.pytorch.org/whl/cu80/torch-0.3.0.post4-cp36-cp36m-linux_x86_64.whl \n",
    "pip install torchvision`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第3章 多层全连接神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### 3.1 热身：PyTorch基础"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### 3.1.1 张量(Tensor)\n",
    "PyTorch里面最基本的操作对象就是Tensor，表示的是一个多维的矩阵，比如零维就是一个点，一维就是向量，二维就是矩阵，多维就是多维数组，这和numpy是对应的，不同的是PyTorch可以再GPU上运行，有着比 NumPy 快很多倍的速度，而numpy只能在CPU上运行。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##### 3.1.1.1 Tensor数据类型\n",
    "不同数据类型的Tensor：\n",
    "- torch.FloatTensor\n",
    "- torch.IntTensor\n",
    "- torch.LongTensor  \n",
    "\n",
    "**torch.Tensor默认的是torch.FloatTensor数据类型**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a is tensor([[2., 3.],\n",
      "        [4., 5.],\n",
      "        [7., 8.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 定义一个Tensor. 默认是FloatTensor类型.\n",
    "a = torch.Tensor([[2,3],[4,5],[7,8]])\n",
    "print('a is {}'.format(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zero tensor: tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]])\n",
      "normal randn tensor: tensor([[-1.7245, -0.8966],\n",
      "        [-0.1098,  1.3504],\n",
      "        [ 1.7103, -0.3634]])\n"
     ]
    }
   ],
   "source": [
    "# 定义一个全0的Tensor.\n",
    "b = torch.zeros((3,2))\n",
    "print('zero tensor: {}'.format(b))\n",
    "\n",
    "# 定义一个正态分布的Tensor.\n",
    "c = torch.randn((3,2))\n",
    "print('normal randn tensor: {}'.format(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##### 3.1.1.3 Tensor的操作\n",
    "\n",
    "- tensor的大小：`x.shape和x.size()`\n",
    "- 判断数据类型：`x.type()`\n",
    "- tensor维度：`x.dim()`\n",
    "- tensor的所有的元素个数：`x.numl()`\n",
    "- 强制数据类型转化：`x.long(), x.float()`\n",
    "- 沿着行取最大值：`torch.max(x, dim=1)`\n",
    "- 沿着行对Tensor求和：`torch.sum(x, dim=1)`\n",
    "- 增加维度或者减少维度：`x.unsqueeze(?)与x.squeeze(?)`\n",
    "- 维度交换：`x.permute()和x.transpose()`\n",
    "- 维度变换：`x.view()`  \n",
    "\n",
    "\n",
    "**pytorch中大多数的操作都支持 inplace 操作，也就是可以直接对 tensor 进行操作而不需要另外开辟内存空间，方式非常简单，一般都是在操作的符号后面加`_`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2])\n",
      "torch.Size([3, 2])\n",
      "torch.LongTensor\n",
      "2\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "# 可以通过下面两种方式得到 tensor 的大小\n",
    "print(a.shape)\n",
    "print(a.size())\n",
    "\n",
    "# 得到 tensor 的数据类型\n",
    "print(a.type())\n",
    "\n",
    "# 得到 tensor 的维度\n",
    "print(a.dim())\n",
    "\n",
    "# 得到 tensor 的所有元素个数\n",
    "print(a.numel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2, 3],\n",
      "        [4, 5],\n",
      "        [7, 8]])\n"
     ]
    }
   ],
   "source": [
    "# 将其转化为整形\n",
    "a = a.long()\n",
    "# a = a.type(torch.LongTensor)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "每一行的最大值: tensor([-0.8966,  1.3504,  1.7103])\n",
      "每一行最大值的下标: tensor([1, 1, 0])\n"
     ]
    }
   ],
   "source": [
    "# 沿着行取最大值\n",
    "max_value, max_idx = torch.max(c, dim=1)\n",
    "print('每一行的最大值:', max_value)\n",
    "print('每一行最大值的下标:', max_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-2.6211,  1.2406,  1.3469])\n"
     ]
    }
   ],
   "source": [
    "# 沿着行对 x 求和\n",
    "sum_c = torch.sum(c, dim=1)\n",
    "print(sum_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2])\n",
      "在第一维增加 torch.Size([1, 3, 2])\n",
      "在第二维增加 torch.Size([1, 1, 3, 2])\n",
      "减少第一维 torch.Size([1, 3, 2])\n",
      "将 tensor 中所有的一维全部都去掉 torch.Size([3, 2])\n"
     ]
    }
   ],
   "source": [
    "# 增加维度或者减少维度\n",
    "print(c.shape)\n",
    "\n",
    "c = c.unsqueeze(0) # 在第一维增加\n",
    "print('在第一维增加：', c.shape)\n",
    "\n",
    "c = c.unsqueeze(1) # 在第二维增加\n",
    "print('在第二维增加：', c.shape)\n",
    "\n",
    "c = c.squeeze(0) # 减少第一维\n",
    "print('减少第一维：', c.shape)\n",
    "\n",
    "c = c.squeeze() # 将 tensor 中所有的一维全部都去掉\n",
    "print('将 tensor 中所有的一维全部都去掉：', c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4, 5])\n",
      "按照1-0-2的顺序重排Tensor： torch.Size([4, 3, 5])\n",
      "交换0与2维： torch.Size([5, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, 4, 5)\n",
    "print(x.shape)\n",
    "\n",
    "x = x.permute(1, 0, 2) # permute 可以重新排列 tensor 的维度\n",
    "print('按照1-0-2的顺序重排Tensor：', x.shape)\n",
    "\n",
    "x = x.transpose(0, 2)  # transpose 交换 tensor 中的两个维度\n",
    "print('交换0与2维：', x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4, 5])\n",
      "torch.Size([12, 5])\n",
      "torch.Size([3, 20])\n"
     ]
    }
   ],
   "source": [
    "# 使用 view 对 tensor 进行 reshape\n",
    "x = torch.randn(3, 4, 5)\n",
    "print(x.shape)\n",
    "\n",
    "x = x.view(-1, 5) # -1 表示任意的大小，5 表示第二维变成 5\n",
    "print(x.shape)\n",
    "\n",
    "x = x.view(3, 20) # 重新 reshape 成 (3, 20) 的大小\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "x = torch.randn(3, 4)\n",
    "y = torch.randn(3, 4)\n",
    "\n",
    "# 两个 tensor 求和\n",
    "z = x + y\n",
    "# z = torch.add(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 3])\n",
      "torch.Size([1, 3, 3])\n",
      "torch.Size([3, 1, 3])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(3, 3)\n",
    "print(x.shape)\n",
    "\n",
    "# unsqueeze 进行 inplace\n",
    "x.unsqueeze_(0)\n",
    "print(x.shape)\n",
    "\n",
    "# transpose 进行 inplace\n",
    "x.transpose_(1, 0)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[2., 2., 2.],\n",
      "        [2., 2., 2.],\n",
      "        [2., 2., 2.]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(3, 3)\n",
    "y = torch.ones(3, 3)\n",
    "print(x)\n",
    "\n",
    "# add 进行 inplace\n",
    "x.add_(y)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##### 3.1.1.2 Tensor使用GPU加速\n",
    "我们可以使用以下两种方式将 Tensor 放到 GPU 上：\n",
    "- 使用第一种方式将 tensor 放到 GPU 上的时候会将数据类型转换成定义的类型，而是用第二种方式能够直接将 tensor 放到 GPU 上，类型跟之前保持一致\n",
    "\n",
    "- 推荐在定义 tensor 的时候就明确数据类型，然后直接使用第二种方法将 tensor 放到 GPU 上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# 第一种方式是定义 cuda 数据类型.\n",
    "dtype = torch.cuda.FloatTensor # 定义默认 GPU 的 数据类型\n",
    "gpu_tensor = torch.randn(10, 20).type(dtype)\n",
    "\n",
    "# 第二种方式更简单，推荐使用.\n",
    "gpu_tensor = torch.randn(10, 20).cuda(0) # 将 tensor 放到第一个 GPU 上\n",
    "gpu_tensor = torch.randn(10, 20).cuda(1) # 将 tensor 放到第二个 GPU 上"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "将 tensor 放回 CPU 的操作非常简单: `x.cpu()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cpu_tensor = gpu_tensor.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### 3.1.1.3 Tensor 与 NumPy ndarray\n",
    "\n",
    "使用下面两种方式将numpy的ndarray转换到tensor上:  \n",
    "- `torch.Tensor()`\n",
    "- `torch.from_numpy()`\n",
    "\n",
    "使用下面的方法将 pytorch tensor 转换为 numpy ndarray:  \n",
    "- `x.numpy()`  \n",
    "\n",
    "**需要注意 GPU 上的 Tensor 不能直接转换为 NumPy ndarray，需要使用`.cpu()`先将 GPU 上的 Tensor 转到 CPU 上**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 创建一个 numpy ndarray\n",
    "numpy_tensor = np.random.randn(10, 20)\n",
    "\n",
    "pytorch_tensor1 = torch.Tensor(numpy_tensor)\n",
    "pytorch_tensor2 = torch.from_numpy(numpy_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# 如果 pytorch tensor 在 cpu 上\n",
    "numpy_array = pytorch_tensor1.numpy()\n",
    "\n",
    "# 如果 pytorch tensor 在 gpu 上\n",
    "numpy_array = pytorch_tensor1.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### 3.1.2 Variable (变量)\n",
    "\n",
    "Variable和Tensor本质上没有区别，不过Variable会被放入一个计算图中，然后进行前向传播，反向传播，自动求导。  \n",
    "\n",
    "首先，Variable是在 `torch.autograd.Variable` 中。  \n",
    "\n",
    "Variable 是对 tensor 的封装，操作和 tensor 是一样的，但是每个 Variabel都有三个属性，Variable 中的 tensor本身`.data`，对应 tensor 的反向传播梯度`.grad`以及这个 Variable 是通过什么方式得到的`.grad_fn`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# 通过下面这种方式导入 Variable\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "x_tensor = torch.randn(10, 5)\n",
    "y_tensor = torch.randn(10, 5)\n",
    "\n",
    "# 将 tensor 变成 Variable\n",
    "x = Variable(x_tensor, requires_grad=True) # 默认 Variable 是不需要求梯度的，所以我们用这个方式申明需要对其进行求梯度\n",
    "y = Variable(y_tensor, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "z = torch.sum(x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-5.5860)\n",
      "<SumBackward0 object at 0x0000021350F08550>\n"
     ]
    }
   ],
   "source": [
    "print(z.data)     # 打出了 z 中的 tensor 数值\n",
    "print(z.grad_fn)  # tensor是通过 Sum 这种方式得到的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.]])\n",
      "tensor([[1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# 求 x 和 y 的梯度\n",
    "z.backward()\n",
    "\n",
    "print(x.grad)  # 通过`.grad`我们得到了 x 和 y 的梯度，这里我们使用了 PyTorch 提供的自动求导机制\n",
    "print(y.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### 3.1.3 Dataset (数据集)\n",
    "\n",
    "在处理任何机器学习问题之前都需要数据读取，并进行预处理。Pytorch提供了很多工具使得数据的读取和预处理变得很容易。  \n",
    "\n",
    "`torch.utils.data.Dataset`是代表这一数据的抽象类，你可以自己定义你的数据类继承和重写这个抽象类，只需要定义`__len__`和`__getitem__`这两个函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class myDataset(Dataset):\n",
    "    def __init__(self, csv_file, text_file, root_dir, other_file):\n",
    "        self.csv_data = pd.read_csv(csv_file)\n",
    "        with open(txt_file, 'r') as f:\n",
    "            data_list = f.readlines()\n",
    "        self.txt_data = data_list\n",
    "        self.root_dir = root_dir\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.csv_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        data = (self.csv_data[idx], self.txt_data[idx])\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "通过上面的方式，可以定义我们需要的数据类，可以通过迭代的方式来取得每一个数据，但是这样很难实现取batch，shuffle或者是多线程去读取数据。  \n",
    "\n",
    "PyTorch通过`torch.utils.data.DataLoader`来定义一个新的迭代器：  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dataiter = DataLoader(myDataset, batch_size=32, shuffle=True, collate_fn=default_collate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "另外，在`torchvision`这个包中还有一个更高级的有关于计算机视觉的数据读取类，`ImageFolder`，主要功能是处理图片："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dset = ImageFolder(root='root_path', transform=None, loader=dafault_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### 3.1.4 nn.Module (模组)\n",
    "\n",
    "在PyTorch里面编写神经网络，所有的层结构和损失函数都来自于`torch.nn`，所有的模型构建都是从这个基类`nn.Module`继承的。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# 神经网络模板类。\n",
    "class net_name(nn.Module):\n",
    "    def __init__(self, other_arguments):\n",
    "        super(net_name, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size)\n",
    "        # other network layer\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "这样就建立了一个计算图，并且这个结构可以复用多次，每次调用就相当于用该计算图定义的相同参数做一次前向传播，这得益于PyTorch的自动求导功能，所以我们不需要自己编写反向传播。  \n",
    "\n",
    "定义完模型，我们需要通过nn这个包来定义损失函数。常见的损失函数都已经定义在nn中，比如均方误差、多分类的交叉熵，以及二分类的交叉熵，等等，直接调用这些已经定义好的损失函数即可：  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "loss = criterion(output, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### 3.1.5 torch.optim (优化)\n",
    "\n",
    "在机器学习或者深度学习中，我们需要通过修改参数使得损失函数最小化 (或最大化)，优化算法是一种调整模型参数更新的策略。  \n",
    "\n",
    "优化算法分为两大类：  \n",
    "\n",
    "1. 一阶优化算法  \n",
    "\n",
    "这种算法使用各个参数的梯度值来更新参数，最常用的一阶优化算法是**梯度下降**。**梯度下降**的功能是通过寻找最小值，控制方差，更新模型参数，最终使模型收敛，网络的参数更新公式是：  \n",
    "$$\n",
    "\\theta = \\theta - \\eta \\times \\frac{\\partial J(\\theta)}{\\partial \\theta}\n",
    "$$\n",
    "\n",
    "$\\eta$是学习率，$\\frac{\\partial J(\\theta)}{\\partial \\theta}$是函数的梯度\n",
    "\n",
    "2. 二阶优化算法  \n",
    "\n",
    "二阶优化算法使用了二阶导数来最小化或者最大化损失函数，主要基于牛顿法，但是由于二阶导数的计算成本很高，所以这种方法并没有广泛使用。  \n",
    "\n",
    "`torch.optim`是一个实现各种优化算法的包，在调用的时候将需要优化的参数传入，这些参数都必须是Variable，然后传入一些基本的设定，比如学习率和动量等。  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "在优化之前需要先将梯度归零，即`optimizer.zeros()`，然后通过`loss.backward()`反向传播，自动求导得到每个参数的梯度，最后只需要`optimizer.step()`就可以通过梯度做一步参数更新。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### 3.1.6 模型的保存和加载\n",
    "\n",
    "在PyTorch里面使用`torch.save`来保存模型的结构和参数。  \n",
    "\n",
    "1. 保存整个模型的结构信息和参数信息，保存的对象是模型model  \n",
    "\n",
    "2. 保存模型的参数，保存的对象是模型的状态 `model.state_dict()`  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "torch.save(model, './model.pth')\n",
    "torch.save(model.state_dict(), './model_state.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "加载模型有两种方式对应于保存模型的方式：  \n",
    "\n",
    "1. 加载完整的模型结构和参数信息，使用`load_model = torch.load('model.pth')`，在网络较大的时候加载的时间比较长，同时存储空间也较大；\n",
    "\n",
    "2. 加载模型参数信息，需要先导入模型的结构，然后通过`model.load_state_dic(torch.load('model_state.pth'))`来导入。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### 3.2 线性模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### 3.2.1 一维线性回归"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### 3.2.2 多项式回归"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### 3.3 分类问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### 3.3.1 Logistic 回归"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### 3.4 简单的多层全连接前向网络\n",
    "\n",
    "前面我们介绍了机器学习领域中两个最基本的算法：一个是线性回归，一个是Logistic回归。  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### 3.4.1 单层神经网络的分类器\n",
    "\n",
    "一层神经网络 = 一个线性运算 + 一个激活函数  \n",
    "\n",
    "Logistic回归 = 使用了Sigmoiduo作为激活函数的一层神经网络  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### 3.4.2 激活函数  \n",
    "\n",
    "1. Sigmoid  \n",
    "\n",
    "将一个实数输入转化到0~1之间的输出，具体来说也就是将越大的负数转化到越靠近0，越大的正数转化到越靠近1。  \n",
    "\n",
    "缺点：  \n",
    "\n",
    "- Sigmoid函数会造成梯度消失。一个非常不好的特点是Sigmoid函数在靠近1和0的两端时，梯度会几乎变成0。梯度下降法通过梯度乘上学习率来更新参数，因此如果梯度接近0，那么没有任何信息来更新参数，这样就造成了模型不收敛。  \n",
    "\n",
    "- Sigmoid输出不是以0为均值。这就会造成经过Sigmoid激活函数之后的输出，作为后面一层网络的输入的时候是非0均值的，这个时候如果输入进入下一层神经元的时候全是正的，这就会导致梯度全是正的，那么在更新参数的时候永远都是正梯度。  \n",
    "\n",
    "\n",
    "2. Tanh  \n",
    "\n",
    "它将输入的数据转化到-1~1之间，它将输出变成了0均值，在一定程度上解决了Sigmoidh函数的第二个问题，但是它仍然存在梯度消失的问题。\n",
    "\n",
    "\n",
    "3. ReLU  \n",
    "\n",
    "只是简单地将大于0的部分保留，将小于0的部分变成0。  \n",
    "\n",
    "优点：  \n",
    "\n",
    "- 相比于Sigmoid激活函数和Tanh激活函数，ReLU激活函数能够极大地加速随机梯度下降法的收敛速度，这是因为它是线性的，且不存在梯度消失的问题。 \n",
    "\n",
    "- 相比于Sigmoid激活函数和Tanh激活函数的复杂计算而言，ReLU的计算方法更简单，只需要一个阈值过滤就可以得到结果，不需要进行一大堆复杂的运算。  \n",
    "\n",
    "缺点：  \n",
    "\n",
    "训练的时候很脆弱，在实际操作中可以通过设置比较小的学习率来避免这个问题\n",
    "\n",
    "4. Leaky ReLU  \n",
    "\n",
    "5. Maxout  \n",
    "\n",
    "有着ReLU激活函数的优点，同时也避免了ReLU激活函数训练脆弱的缺点。不过他有一个缺点，那就是它加倍了模型的参数，导致了模型的存储变大。  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### 3.4.3 神经网络的结构\n",
    "\n",
    "一般而言，N层神经网络并不会把输入层算进来，因此一个一层的神经网络是指没有隐藏层、只有输入层和输出层的神经网路。 \n",
    "\n",
    "输出层一般是没有激活函数的，因为输出层通常表示一个类别的得分或者回归的一个实值的目标，所以输出层可以是任意的实数。  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### 3.5 深度学习的基石：反向传播算法\n",
    "\n",
    "前面我们介绍了三个模型，整个处理的基本流程都是定义模型，读入数据，给出损失函数$f$，通过梯度下降法更新参数。PyTorch 提供了非常简单的自动求导帮助我们求解导数，对于比较简单的模型，我们也能手动求出参数的梯度，但是对于非常复杂的模型，比如一个 100 层的网络，我们如何能够有效地手动求出这个梯度呢？这里就需要引入反向传播算法，自动求导本质是就是一个反向传播算法。  \n",
    "\n",
    "反向传播算法是一个有效地求解梯度的算法，本质上其实就是一个链式求导法则的应用。  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### 3.5.1 链式法则\n",
    "\n",
    "首先来简单地介绍一下链式法则，考虑一个简单的函数，比如\n",
    "$$f(x, y, z) = (x + y)z$$\n",
    "\n",
    "我们当然可以直接求出这个函数的微分，但是这里我们要使用链式法则，令\n",
    "$$q=x+y$$\n",
    "\n",
    "那么\n",
    "\n",
    "$$f = qz$$\n",
    "\n",
    "对于这两个式子，我们可以分别求出他们的微分 \n",
    "\n",
    "$$\\frac{\\partial f}{\\partial q} = z, \\frac{\\partial f}{\\partial z}=q$$\n",
    "\n",
    "同时$q$是$x$和$y$的求和，所以我们能够得到\n",
    "\n",
    "$$\\frac{\\partial q}{x} = 1, \\frac{\\partial q}{y} = 1$$\n",
    "\n",
    "我们关心的问题是\n",
    "\n",
    "$$\\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y}, \\frac{\\partial f}{\\partial z}$$\n",
    "\n",
    "链式法则告诉我们如何来计算出他们的值\n",
    "\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial x} = \\frac{\\partial f}{\\partial q}\\frac{\\partial q}{\\partial x}\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial y} = \\frac{\\partial f}{\\partial q}\\frac{\\partial q}{\\partial y}\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial z} = q\n",
    "$$\n",
    "\n",
    "通过链式法则我们知道如果我们需要对其中的元素求导，那么我们可以一层一层求导然后将结果乘起来，这就是链式法则的核心，也是反向传播算法的核心\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### 3.5.2 反向传播算法\n",
    "\n",
    "了解了链式法则，我们就可以开始介绍反向传播算法了，本质上反向传播算法只是链式法则的一个应用。我们还是使用之前那个相同的例子$q=x+y, f=qz$，通过计算图可以将这个计算过程表达出来\n",
    "\n",
    "![](https://ws1.sinaimg.cn/large/006tNc79ly1fmiozcinyzj30c806vglk.jpg)\n",
    "\n",
    "上面绿色的数字表示其数值，下面红色的数字表示求出的梯度，我们可以一步一步看看反向传播算法的实现。首先从最后开始，梯度当然是1，然后计算\n",
    "\n",
    "$$\\frac{\\partial f}{\\partial q} = z = -4,\\ \\frac{\\partial f}{\\partial z} = q = 3$$\n",
    "\n",
    "接着我们计算\n",
    "$$\\frac{\\partial f}{\\partial x} = \\frac{\\partial f}{\\partial q} \\frac{\\partial q}{\\partial x} = -4 \\times 1 = -4,\\ \\frac{\\partial f}{\\partial y} = \\frac{\\partial f}{\\partial q} \\frac{\\partial q}{\\partial y} = -4 \\times 1 = -4$$\n",
    "\n",
    "这样一步一步我们就求出了$\\nabla f(x, y, z)$。\n",
    "\n",
    "直观上看反向传播算法是一个优雅的局部过程，每次求导只是对当前的运算求导，求解每层网络的参数都是通过链式法则将前面的结果求出不断迭代到这一层，所以说这是一个传播过程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### 3.5.3 Sigmoid函数举例\n",
    "\n",
    "下面我们通过Sigmoid函数来演示反向传播过程在一个复杂的函数上是如何进行的。\n",
    "\n",
    "$$\n",
    "f(w, x) = \\frac{1}{1+e^{-(w_0 x_0 + w_1 x_1 + w_2)}}\n",
    "$$\n",
    "\n",
    "我们需要求解出\n",
    "$$\\frac{\\partial f}{\\partial w_0}, \\frac{\\partial f}{\\partial w_1}, \\frac{\\partial f}{\\partial w_2}$$\n",
    "\n",
    "首先我们将这个函数抽象成一个计算图来表示，即\n",
    "$$\n",
    "   f(x) = \\frac{1}{x} \\\\\n",
    "   f_c(x) = 1 + x \\\\\n",
    "   f_e(x) = e^x \\\\\n",
    "   f_w(x) = -(w_0 x_0 + w_1 x_1 + w_2)\n",
    "$$\n",
    "\n",
    "这样我们就能够画出下面的计算图\n",
    "\n",
    "![](https://ws1.sinaimg.cn/large/006tNc79ly1fmip1va5qjj30lb08e0t0.jpg)\n",
    "\n",
    "同样上面绿色的数子表示数值，下面红色的数字表示梯度，我们从后往前计算一下各个参数的梯度。首先最后面的梯度是1,，然后经过$\\frac{1}{x}$这个函数，这个函数的梯度是$-\\frac{1}{x^2}$，所以往前传播的梯度是$1 \\times -\\frac{1}{1.37^2} = -0.53$，然后是$+1$这个操作，梯度不变，接着是$e^x$这个运算，它的梯度就是$-0.53 \\times e^{-1} = -0.2$，这样不断往后传播就能够求得每个参数的梯度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 各种优化算法的变式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### 3.6.1 梯度下降法\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### 3.6.2 梯度下降法的变式\n",
    "\n",
    "**1. SGD** \n",
    "\n",
    "随机梯度下降法是梯度下降法的一个小变形，就是每次使用一批batch数据进行梯度计算，而不是计算全部数据的梯度。容易跳出局部极小点。  \n",
    "\n",
    "随机梯度下降法非常简单，公式就是\n",
    "$$\n",
    "\\theta_{i+1} = \\theta_i - \\eta \\nabla L(\\theta)\n",
    "$$\n",
    "\n",
    "\n",
    "**2. Momentum**  \n",
    "\n",
    "在随机梯度下降的同时，增加动量(momentum)。  \n",
    "\n",
    "梯度下降法的问题\n",
    "考虑一个二维输入，$[x_1, x_2]$，输出的损失函数 $L: R^2 \\rightarrow R$，下面是这个函数的等高线\n",
    "\n",
    "![](https://ws1.sinaimg.cn/large/006tKfTcly1fmnketw5f4j30az04lq31.jpg)\n",
    "\n",
    "可以想象成一个很扁的漏斗，这样在竖直方向上，梯度就非常大，在水平方向上，梯度就相对较小，所以我们在设置学习率的时候就不能设置太大，为了防止竖直方向上参数更新太过了，这样一个较小的学习率又导致了水平方向上参数在更新的时候太过于缓慢，所以就导致最终收敛起来非常慢。\n",
    "\n",
    "\n",
    "动量法的提出就是为了应对这个问题，我们梯度下降法做一个修改如下\n",
    "\n",
    "$$\n",
    "v_i = \\gamma v_{i-1} + \\eta \\nabla L(\\theta)\n",
    "$$\n",
    "$$\n",
    "\\theta_i = \\theta_{i-1} - v_i\n",
    "$$\n",
    "\n",
    "其中 $v_i$ 是当前速度，$\\gamma$ 是动量参数，是一个小于 1的正数，$\\eta$ 是学习率  \n",
    "\n",
    "相当于每次在进行参数更新的时候，都会将之前的速度考虑进来，每个参数在各方向上的移动幅度不仅取决于当前的梯度，还取决于过去各个梯度在各个方向上是否一致，如果一个梯度一直沿着当前方向进行更新，那么每次更新的幅度就越来越大，如果一个梯度在一个方向上不断变化，那么其更新幅度就会被衰减，这样我们就可以使用一个较大的学习率，使得收敛更快，同时梯度比较大的方向就会因为动量的关系每次更新的幅度减少，如下图\n",
    "\n",
    "![](https://ws1.sinaimg.cn/large/006tNc79gy1fmo5l53o76j30ak04gjrh.jpg)\n",
    "\n",
    "比如我们的梯度每次都等于 g，而且方向都相同，那么动量法在该方向上使参数加速移动，有下面的公式：\n",
    "\n",
    "$$\n",
    "v_0 = 0\n",
    "$$\n",
    "$$\n",
    "v_1 = \\gamma v_0 + \\eta g = \\eta g\n",
    "$$\n",
    "$$\n",
    "v_2 = \\gamma v_1 + \\eta g = (1 + \\gamma) \\eta g\n",
    "$$\n",
    "$$\n",
    "v_3 = \\gamma v_2 + \\eta g = (1 + \\gamma + \\gamma^2) \\eta g\n",
    "$$\n",
    "$$\n",
    "\\cdots\n",
    "$$\n",
    "$$\n",
    "v_{+ \\infty} = (1 + \\gamma + \\gamma^2 + \\gamma^3 + \\cdots) \\eta g = \\frac{1}{1 - \\gamma} \\eta g\n",
    "$$\n",
    "\n",
    "如果我们把 $\\gamma$ 定为 0.9，那么更新幅度的峰值就是原本梯度乘学习率的 10 倍。\n",
    "\n",
    "本质上说，动量法就仿佛我们从高坡上推一个球，小球在向下滚动的过程中积累了动量，在途中也会变得越来越快，最后会达到一个峰值，对应于我们的算法中就是，动量项会沿着梯度指向方向相同的方向不断增大，对于梯度方向改变的方向逐渐减小，得到了更快的收敛速度以及更小的震荡。\n",
    "\n",
    "\n",
    "**3. Adagrad**\n",
    "\n",
    "这个优化算法被称为自适应学习率优化算法，之前我们讲的随机梯度下降以及动量法对所有的参数都使用的固定的学习率进行参数更新，但是不同的参数梯度可能不一样，所以需要不同的学习率才能比较好的进行训练，但是这个事情又不能很好地被人为操作，所以 Adagrad 便能够帮助我们做这件事。  \n",
    "\n",
    "Adagrad 的想法非常简答，在每次使用一个 batch size 的数据进行参数更新的时候，我们需要计算所有参数的梯度，那么其想法就是对于每个参数，初始化一个变量 s 为 0，然后每次将该参数的梯度平方求和累加到这个变量 s 上，然后在更新这个参数的时候，学习率就变为\n",
    "\n",
    "$$\n",
    "\\frac{\\eta}{\\sqrt{s + \\epsilon}}\n",
    "$$\n",
    "\n",
    "这里的 $\\epsilon$ 是为了数值稳定性而加上的，因为有可能 s 的值为 0，那么 0 出现在分母就会出现无穷大的情况，通常 $\\epsilon$ 取 $10^{-10}$，这样不同的参数由于梯度不同，他们对应的 s 大小也就不同，所以上面的公式得到的学习率也就不同，这也就实现了自适应的学习率。  \n",
    "\n",
    "Adagrad 的核心想法就是，如果一个参数的梯度一直都非常大，那么其对应的学习率就变小一点，防止震荡，而一个参数的梯度一直都非常小，那么这个参数的学习率就变大一点，使得其能够更快地更新\n",
    "\n",
    "Adagrad 也有一些问题，因为 s 不断累加梯度的平方，所以会越来越大，导致学习率在后期会变得较小，导致收敛乏力的情况，可能无法收敛到表较好的结果。  \n",
    "\n",
    "\n",
    "**4. RMSprop**\n",
    "\n",
    "RMSprop 是由 Geoff Hinton 在他 Coursera 课程中提出的一种适应性学习率方法，至今仍未被公开发表。前面我们提到了 Adagrad 算法有一个问题，就是学习率分母上的变量 s 不断被累加增大，最后会导致学习率除以一个比较大的数之后变得非常小，这不利于我们找到最后的最优解，所以 RMSProp 的提出就是为了解决这个问题。\n",
    "\n",
    "RMSProp 仍然会使用梯度的平方量，不同于 Adagrad，其会使用一个指数加权移动平均来计算这个 s，也就是\n",
    "\n",
    "$$\n",
    "s_i = \\alpha s_{i-1} + (1 - \\alpha) \\ g^2\n",
    "$$\n",
    "\n",
    "这里 g 表示当前求出的参数梯度，然后最终更新和 Adagrad 是一样的，学习率变成了\n",
    "\n",
    "$$\n",
    "\\frac{\\eta}{\\sqrt{s + \\epsilon}}\n",
    "$$\n",
    "\n",
    "这里 $\\alpha$ 是一个移动平均的系数，也是因为这个系数，导致了 RMSProp 和 Adagrad 不同的地方，这个系数使得 RMSProp 更新到后期累加的梯度平方较小，从而保证 s 不会太大，也就使得模型后期依然能够找到比较优的结果。\n",
    "\n",
    "\n",
    "**5. Adam**\n",
    "\n",
    "Adam 是一个结合了动量法和 RMSProp 的优化算法，其结合了两者的优点。\n",
    "\n",
    "Adam 算法会使用一个动量变量 v 和一个 RMSProp 中的梯度元素平方的移动指数加权平均 s，首先将他们全部初始化为 0，然后在每次迭代中，计算他们的移动加权平均进行更新\n",
    "\n",
    "$$\n",
    "v = \\beta_1 v + (1 - \\beta_1) g \\\\\n",
    "s = \\beta_2 s + (1 - \\beta_2) g^2\n",
    "$$\n",
    "\n",
    "在 adam 算法里，为了减轻 v 和 s 被初始化为 0 的初期对计算指数加权移动平均的影响，每次 v 和 s 都做下面的修正\n",
    "\n",
    "$$\n",
    "\\hat{v} = \\frac{v}{1 - \\beta_1^t} \\\\\n",
    "\\hat{s} = \\frac{s}{1 - \\beta_2^t}\n",
    "$$\n",
    "\n",
    "这里 t 是迭代次数，可以看到，当 $0 \\leq \\beta_1, \\beta_2 \\leq 1$ 的时候，迭代到后期 t 比较大，那么 $\\beta_1^t$ 和 $\\beta_2^t$ 就几乎为 0，就不会对 v 和 s 有任何影响了，算法作者建议$\\beta_1 = 0.9$, $\\beta_2 = 0.999$。\n",
    "\n",
    "最后使用修正之后的 $\\hat{v}$ 和 $\\hat{s}$ 进行学习率的重新计算\n",
    "\n",
    "$$\n",
    "g' = \\frac{\\eta \\hat{v}}{\\sqrt{\\hat{s} + \\epsilon}}\n",
    "$$\n",
    "\n",
    "这里 $\\eta$ 是学习率，$epsilon$ 仍然是为了数值稳定性而添加的常数，最后参数更新有\n",
    "\n",
    "$$\n",
    "\\theta_i = \\theta_{i-1} - g'\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7 处理数据和训练模型的技巧\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.8 多层全连接神经网络实现MNIST手写数字分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
